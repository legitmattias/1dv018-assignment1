{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Algorithms and Data Structures Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs empirical analysis of algorithm performance to verify theoretical Big O complexity predictions. We test two fundamental algorithmic problems with multiple implementations to demonstrate how different approaches scale with input size.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook tests and visualizes the performance of:\n",
    "\n",
    "**UnionFind Algorithms** (4 implementations):\n",
    "- **Quick Find**: O(1) find, O(N) union - simple but inefficient for many unions\n",
    "- **Quick Union**: O(N) find, O(1) union - better for sparse operations\n",
    "- **Weighted Quick Union**: O(log N) find, O(log N) union - balanced approach with tree size tracking\n",
    "- **Weighted Quick Union with Path Compression**: O(α(N)) amortized - optimal performance\n",
    "- Tested with varying input sizes (1K-100K elements) and proportional operations\n",
    "\n",
    "**3Sum Algorithms** (3 implementations):\n",
    "- **Brute Force**: O(N³) - checks all possible triplets\n",
    "- **Optimized Two Pointers**: O(N²) - sorts array and uses two-pointer technique\n",
    "- **Hash Set**: O(N²) - uses hash table for constant-time lookups\n",
    "- Tested with different array sizes (80-8K elements)\n",
    "\n",
    "**Analysis Methodology**:\n",
    "- Performance plots comparing execution times across input sizes\n",
    "- Log-log scale plots to verify Big O complexity slopes (slope = complexity exponent)\n",
    "- Statistical timing measurements using %timeit for accuracy\n",
    "- Complexity verification through slope analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add src directory to path to import custom algorithm implementations\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Import custom algorithm implementations\n",
    "from threesum import (\n",
    "    generate_test_data,\n",
    "    three_sum_brute_force,\n",
    "    three_sum_optimized,\n",
    "    three_sum_optimized_with_hash,\n",
    ")\n",
    "from unionfind import (\n",
    "    QuickFind,\n",
    "    QuickUnion,\n",
    "    WeightedQuickUnion,\n",
    "    WeightedQuickUnionPathCompression,\n",
    ")\n",
    "\n",
    "# Configure plotting style for pretty-looking graphs\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducible results across runs for consistency\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for performance testing and data generation\n",
    "\n",
    "\n",
    "def setup_unionfind_test(uf_class, n: int, operations: list[tuple[int, int]]):\n",
    "    \"\"\"\n",
    "    Setup UnionFind instance and operations for %timeit testing.\n",
    "\n",
    "    This function creates a closure that contains the UnionFind instance\n",
    "    and all operations to be performed, allowing %timeit to measure\n",
    "    just the algorithm execution time without setup overhead.\n",
    "    \"\"\"\n",
    "    uf = uf_class(n)\n",
    "\n",
    "    def run_operations():\n",
    "        for p, q in operations:\n",
    "            uf.union(p, q)\n",
    "\n",
    "    return run_operations\n",
    "\n",
    "\n",
    "def generate_unionfind_operations(n: int, num_operations: int) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Generate random union operations for testing.\n",
    "\n",
    "    Creates a list of random (p, q) pairs where both p and q are\n",
    "    valid indices in the range [0, n-1]. The number of operations\n",
    "    is typically proportional to n to ensure meaningful performance\n",
    "    comparisons across different input sizes.\n",
    "    \"\"\"\n",
    "    operations = []\n",
    "    for _ in range(num_operations):\n",
    "        p = random.randint(0, n - 1)\n",
    "        q = random.randint(0, n - 1)\n",
    "        operations.append((p, q))\n",
    "    return operations\n",
    "\n",
    "\n",
    "def setup_threesum_test(func, nums: list[int]):\n",
    "    \"\"\"\n",
    "    Setup 3Sum function for %timeit testing.\n",
    "\n",
    "    Creates a closure that contains the test data and function\n",
    "    to be tested, allowing %timeit to measure just the algorithm\n",
    "    execution time without data generation overhead.\n",
    "\n",
    "    Uses value-based deduplication to avoid inflated result counts.\n",
    "    \"\"\"\n",
    "\n",
    "    def run_threesum():\n",
    "        return func(nums, return_values=True)\n",
    "\n",
    "    return run_threesum\n",
    "\n",
    "\n",
    "print(\"Performance measurement functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UnionFind Performance Analysis\n",
    "# =============================================================================\n",
    "# This section tests the performance of four different UnionFind implementations\n",
    "# across varying input sizes to verify their theoretical time complexities.\n",
    "\n",
    "print(\"=== UnionFind Performance Analysis ===\")\n",
    "\n",
    "# Test parameters: Input sizes from 1K to 100K elements\n",
    "# These sizes are chosen to show clear performance differences between algorithms\n",
    "# and allow for meaningful slope analysis in log-log plots\n",
    "n_values = [1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "# UnionFind algorithms to test - ordered from least to most efficient\n",
    "uf_algorithms = [\n",
    "    (\"Quick Find\", QuickFind),  # O(1) find, O(N) union\n",
    "    (\"Quick Union\", QuickUnion),  # O(N) find, O(1) union\n",
    "    (\"Weighted Quick Union\", WeightedQuickUnion),  # O(log N) find, O(log N) union\n",
    "    (\n",
    "        \"Weighted Quick Union with Path Compression\",\n",
    "        WeightedQuickUnionPathCompression,\n",
    "    ),  # O(α(N)) amortized\n",
    "]\n",
    "\n",
    "# Store performance results for analysis and visualization\n",
    "uf_results = []\n",
    "\n",
    "# Test each algorithm with each input size\n",
    "for n in n_values:\n",
    "    # Scale operations proportionally to N\n",
    "    # 0.9 ratio ensures sufficient operations while avoiding complete connectivity\n",
    "    # This creates a realistic workload where most elements get connected\n",
    "    num_operations = int(0.9 * n)\n",
    "    print(f\"\\nTesting with N = {n}, Operations = {num_operations}\")\n",
    "    operations = generate_unionfind_operations(n, num_operations)\n",
    "\n",
    "    # Test each UnionFind implementation with the same set of operations\n",
    "    for name, uf_class in uf_algorithms:\n",
    "        test_func = setup_unionfind_test(uf_class, n, operations)\n",
    "\n",
    "        # Use %timeit for accurate timing measurements\n",
    "        # -q flag suppresses output, -o flag returns timing object\n",
    "        result = %timeit -q -o test_func() # pyright: ignore\n",
    "\n",
    "        # Store results for later analysis\n",
    "        uf_results.append(\n",
    "            {\n",
    "                \"Algorithm\": name,\n",
    "                \"N\": n,\n",
    "                \"Operations\": num_operations,\n",
    "                \"Time (s)\": result.best,  # Best time from multiple runs\n",
    "                \"Average (s)\": result.average,  # Average time from multiple runs\n",
    "            }\n",
    "        )\n",
    "        print(f\"  {name}: {result.best:.6f}s (best), {result.average:.6f}s (avg)\")\n",
    "\n",
    "print(f\"\\nUnionFind analysis completed! {len(uf_results)} measurements taken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3Sum Performance Analysis\n",
    "# =============================================================================\n",
    "# This section tests three different approaches to the 3Sum problem:\n",
    "# 1. Brute Force: O(N³) - checks all possible triplets\n",
    "# 2. Optimized Two Pointers: O(N²) - sorts array and uses two-pointer technique\n",
    "# 3. Hash Set: O(N²) - uses hash table for constant-time lookups\n",
    "#\n",
    "# NOTE: All algorithms now use value-based deduplication (return_values=True)\n",
    "# to avoid inflated result counts from duplicate values in random test data.\n",
    "\n",
    "print(\"=== 3Sum Performance Analysis ===\")\n",
    "\n",
    "# Test parameters for 3Sum algorithms\n",
    "# Different array sizes are used for different algorithms based on their complexity:\n",
    "# - Brute force: smaller sizes (80-400) due to O(N³) complexity\n",
    "# - Optimized algorithms: larger sizes (500-8000) due to O(N²) complexity\n",
    "array_sizes_brute = [80, 120, 200, 300, 400]  # Smaller sizes for O(N³) algorithm\n",
    "array_sizes_optimized = [\n",
    "    500,\n",
    "    1000,\n",
    "    2000,\n",
    "    5000,\n",
    "    8000,\n",
    "]  # Larger sizes for O(N²) algorithms\n",
    "\n",
    "# 3Sum algorithms to test - ordered by theoretical efficiency\n",
    "threesum_algorithms = [\n",
    "    (\"Brute Force\", three_sum_brute_force),  # O(N³) - checks all triplets\n",
    "    (\"Optimized Two Pointers\", three_sum_optimized),  # O(N²) - two-pointer technique\n",
    "    (\"Hash Set\", three_sum_optimized_with_hash),  # O(N²) - hash table approach\n",
    "]\n",
    "\n",
    "# Store performance results for analysis and visualization\n",
    "threesum_results = []\n",
    "\n",
    "# Test brute force algorithm with smaller array sizes\n",
    "# Due to O(N³) complexity, we use smaller sizes to keep execution time reasonable\n",
    "print(\"\\n--- Testing Brute Force with smaller sizes ---\")\n",
    "for size in array_sizes_brute:\n",
    "    print(f\"\\nTesting Brute Force with array size = {size}\")\n",
    "\n",
    "    # Generate random test data for this array size\n",
    "    test_data = generate_test_data(size)\n",
    "\n",
    "    name, func = threesum_algorithms[0]  # Brute Force algorithm\n",
    "    test_func = setup_threesum_test(func, test_data)\n",
    "\n",
    "    # Run algorithm once to get solution count and verify correctness\n",
    "    sample_result = func(test_data, return_values=True)\n",
    "    result = %timeit -q -o test_func()  # pyright: ignore\n",
    "\n",
    "    # Store results including number of solutions found\n",
    "    threesum_results.append(\n",
    "        {\n",
    "            \"Algorithm\": name,\n",
    "            \"Array Size\": size,\n",
    "            \"Time (s)\": result.best,\n",
    "            \"Average (s)\": result.average,\n",
    "            \"Solutions Found\": len(sample_result),\n",
    "        }\n",
    "    )\n",
    "    print(\n",
    "        f\"  {name}: {result.best:.6f}s (best), \"\n",
    "        f\"{result.average:.6f}s (avg), {len(sample_result)} unique value triplets\"\n",
    "    )\n",
    "\n",
    "# Test optimized algorithms with larger array sizes\n",
    "# Due to O(N²) complexity, test with larger sizes and keeping execution time reasonable\n",
    "print(\"\\n--- Testing Optimized algorithms with larger sizes ---\")\n",
    "for size in array_sizes_optimized:\n",
    "    print(f\"\\nTesting with array size = {size}\")\n",
    "\n",
    "    # Generate random test data for this array size\n",
    "    test_data = generate_test_data(size)\n",
    "\n",
    "    # Test both optimized algorithms (skip brute force)\n",
    "    for name, func in threesum_algorithms[1:]:  # Skip brute force\n",
    "        test_func = setup_threesum_test(func, test_data)\n",
    "\n",
    "        # Run algorithm once to get solution count and verify correctness\n",
    "        sample_result = func(test_data, return_values=True)\n",
    "        result = %timeit -q -o test_func()  # pyright: ignore\n",
    "\n",
    "        # Store results including number of solutions found\n",
    "        threesum_results.append(\n",
    "            {\n",
    "                \"Algorithm\": name,\n",
    "                \"Array Size\": size,\n",
    "                \"Time (s)\": result.best,\n",
    "                \"Average (s)\": result.average,\n",
    "                \"Solutions Found\": len(sample_result),\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"  {name}: {result.best:.6f}s (best), \"\n",
    "            f\"{result.average:.6f}s (avg), {len(sample_result)} unique value triplets\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n3Sum analysis completed! {len(threesum_results)} measurements taken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Power-Law Curve Fitting - Simple Constants Extraction\n",
    "# =============================================================================\n",
    "\n",
    "# Convert results to DataFrames for easier analysis\n",
    "threesum_df = pd.DataFrame(threesum_results)\n",
    "\n",
    "\n",
    "def power_law(x, a, b):\n",
    "    return a * (x**b)\n",
    "\n",
    "\n",
    "def power_law_with_offset(x, a, b, c):\n",
    "    \"\"\"Power law with constant offset: c + a * x^b\"\"\"\n",
    "    return c + a * (x**b)\n",
    "\n",
    "\n",
    "print(\"=== Power-Law Constants: time ≈ a·N^b ===\\n\")\n",
    "\n",
    "# Store fitting results for overlay plotting\n",
    "fitted_params = {}\n",
    "\n",
    "MIN_FITTING_POINTS = 2  # Minimum data points required for curve fitting\n",
    "MIN_LARGE_POINTS = 3    # Minimum points for large-size fitting strategy\n",
    "\n",
    "for algorithm in threesum_df[\"Algorithm\"].unique():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm].sort_values(\n",
    "        \"Array Size\"\n",
    "    )\n",
    "    print(f\"\\nFitting {algorithm}:\")\n",
    "    print(f\"  Data points: {len(data)}\")\n",
    "    print(f\"  Size range: {data['Array Size'].min()}-{data['Array Size'].max()}\")\n",
    "    print(\n",
    "        f\"  Time range: {data['Time (s)'].min():.6f}-\"\n",
    "        f\"{data['Time (s)'].max():.6f}\"\n",
    "    )\n",
    "\n",
    "    # Show actual data points to understand the trend\n",
    "    for _, row in data.iterrows():\n",
    "        print(f\"    Size {row['Array Size']:4d}: {row['Time (s)']:.6f}s\")\n",
    "\n",
    "    if len(data) >= MIN_FITTING_POINTS:\n",
    "        # Try simple power law first\n",
    "        try:\n",
    "            (a, b), _ = curve_fit(power_law, data[\"Array Size\"], data[\"Time (s)\"])\n",
    "            simple_fit = (a, b)\n",
    "            print(f\"  Simple fit: a={a:.2e}, b={b:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Simple fitting failed: {e}\")\n",
    "            simple_fit = None\n",
    "\n",
    "        # For Two Pointers, try fitting only larger sizes where scaling is clear\n",
    "        if \"Two Pointers\" in algorithm and len(data) >= MIN_LARGE_POINTS:\n",
    "            try:\n",
    "                # Use only the largest 3 data points where true scaling emerges\n",
    "                large_data = data.tail(MIN_LARGE_POINTS)\n",
    "                print(\n",
    "                    f\"  Trying fit on large sizes only: \"\n",
    "                    f\"{large_data['Array Size'].tolist()}\"\n",
    "                )\n",
    "                (a_large, b_large), _ = curve_fit(\n",
    "                    power_law, large_data[\"Array Size\"], large_data[\"Time (s)\"]\n",
    "                )\n",
    "                print(f\"  Large-size fit: a={a_large:.2e}, b={b_large:.2f}\")\n",
    "\n",
    "                # Use the large-size fit if it has better scaling behavior\n",
    "                if b_large > b if simple_fit else True:\n",
    "                    fitted_params[algorithm] = (a_large, b_large, 'large_only')\n",
    "                    print(\"  → Using large-size fit (better scaling)\")\n",
    "                elif simple_fit:\n",
    "                    fitted_params[algorithm] = (a, b, 'simple')\n",
    "                    print(\"  → Using simple fit\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Large-size fitting failed: {e}\")\n",
    "                if simple_fit:\n",
    "                    fitted_params[algorithm] = (a, b, 'simple')\n",
    "        elif simple_fit:\n",
    "            fitted_params[algorithm] = (a, b, 'simple')\n",
    "\n",
    "print(f\"\\nSuccessfully fitted {len(fitted_params)} algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Performance Visualization with Fitted Curve Overlays\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== Creating Performance Visualizations ===\")\n",
    "\n",
    "# Convert UnionFind results to DataFrame for easier plotting and analysis\n",
    "uf_df = pd.DataFrame(uf_results)\n",
    "\n",
    "# Slope verification for complexity analysis\n",
    "print(\"\\n=== Slope Analysis for Complexity Verification ===\")\n",
    "\n",
    "# Essential constants for linting compliance\n",
    "MIN_DATA_POINTS = 2  # Minimum data points required for slope analysis\n",
    "LARGE_SIZE_TAIL = 3  # Number of largest points to use for large-size fits\n",
    "FIT_TYPE_INDEX = 2  # Index for fit type in fitted_params tuple\n",
    "\n",
    "# Meaningful constants for code maintainability\n",
    "LINE_WIDTH = 2  # Line width for data plots\n",
    "FIT_LINE_WIDTH = 1.5  # Line width for fitted curves\n",
    "FIT_ALPHA = 0.8  # Alpha transparency for fitted curves\n",
    "MARKER_SIZE_SMALL = 5  # Small marker size\n",
    "MARKER_SIZE_MEDIUM = 6  # Medium marker size\n",
    "MARKER_SIZE_LARGE = 7  # Large marker size\n",
    "GRID_ALPHA = 0.3  # Grid transparency\n",
    "FIT_POINTS = 100  # Number of points for full-range fits\n",
    "PARTIAL_FIT_POINTS = 50  # Number of points for partial-range fits\n",
    "\n",
    "# UnionFind slope analysis\n",
    "print(\"\\nUnionFind Complexity Verification:\")\n",
    "for algorithm in uf_df[\"Algorithm\"].unique():\n",
    "    data = uf_df[uf_df[\"Algorithm\"] == algorithm].sort_values(\"N\")\n",
    "    if len(data) >= MIN_DATA_POINTS:\n",
    "        log_n = np.log10(data[\"N\"].values)\n",
    "        log_time = np.log10(data[\"Time (s)\"].values)\n",
    "        slope = (log_time[-1] - log_time[0]) / (log_n[-1] - log_n[0])\n",
    "        print(f\"  {algorithm}: slope = {slope:.2f}\")\n",
    "\n",
    "# 3Sum slope analysis\n",
    "print(\"\\n3Sum Complexity Verification:\")\n",
    "for algorithm in threesum_df[\"Algorithm\"].unique():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm].sort_values(\n",
    "        \"Array Size\"\n",
    "    )\n",
    "    if len(data) >= MIN_DATA_POINTS:\n",
    "        log_size = np.log10(data[\"Array Size\"].values)\n",
    "        log_time = np.log10(data[\"Time (s)\"].values)\n",
    "        slope = (log_time[-1] - log_time[0]) / (log_size[-1] - log_size[0])\n",
    "        print(f\"  {algorithm}: slope = {slope:.2f}\")\n",
    "\n",
    "# Create comprehensive performance visualization with 4 subplots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Define distinctive markers and line styles for each algorithm\n",
    "uf_styles = {\n",
    "    \"Quick Find\": {\n",
    "        \"marker\": \"o\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"markersize\": MARKER_SIZE_MEDIUM\n",
    "    },\n",
    "    \"Quick Union\": {\n",
    "        \"marker\": \"s\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"markersize\": MARKER_SIZE_MEDIUM\n",
    "    },\n",
    "    \"Weighted Quick Union\": {\n",
    "        \"marker\": \"^\",\n",
    "        \"linestyle\": \"-.\",\n",
    "        \"markersize\": MARKER_SIZE_LARGE\n",
    "    },\n",
    "    \"Weighted Quick Union with Path Compression\": {\n",
    "        \"marker\": \"D\",\n",
    "        \"linestyle\": \":\",\n",
    "        \"markersize\": MARKER_SIZE_SMALL\n",
    "    }\n",
    "}\n",
    "\n",
    "threesum_styles = {\n",
    "    \"Brute Force\": {\n",
    "        \"marker\": \"o\",\n",
    "        \"linestyle\": \"-\",\n",
    "        \"markersize\": MARKER_SIZE_MEDIUM\n",
    "    },\n",
    "    \"Optimized Two Pointers\": {\n",
    "        \"marker\": \"s\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"markersize\": MARKER_SIZE_MEDIUM\n",
    "    },\n",
    "    \"Hash Set\": {\n",
    "        \"marker\": \"^\",\n",
    "        \"linestyle\": \"-.\",\n",
    "        \"markersize\": MARKER_SIZE_LARGE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Subplot 1: UnionFind performance on linear scale\n",
    "plt.subplot(2, 2, 1)\n",
    "for algorithm in uf_df[\"Algorithm\"].unique():\n",
    "    data = uf_df[uf_df[\"Algorithm\"] == algorithm]\n",
    "    style = uf_styles[algorithm]\n",
    "    plt.plot(\n",
    "        data[\"N\"], data[\"Time (s)\"], label=algorithm,\n",
    "        marker=style[\"marker\"], linestyle=style[\"linestyle\"],\n",
    "        markersize=style[\"markersize\"], linewidth=LINE_WIDTH\n",
    "    )\n",
    "plt.xlabel(\"Number of Elements (N)\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"UnionFind Performance Comparison\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=GRID_ALPHA)\n",
    "\n",
    "# Subplot 2: UnionFind performance on log-log scale\n",
    "plt.subplot(2, 2, 2)\n",
    "for algorithm in uf_df[\"Algorithm\"].unique():\n",
    "    data = uf_df[uf_df[\"Algorithm\"] == algorithm]\n",
    "    style = uf_styles[algorithm]\n",
    "    plt.loglog(\n",
    "        data[\"N\"], data[\"Time (s)\"], label=algorithm,\n",
    "        marker=style[\"marker\"], linestyle=style[\"linestyle\"],\n",
    "        markersize=style[\"markersize\"], linewidth=LINE_WIDTH\n",
    "    )\n",
    "plt.xlabel(\"Number of Elements (N) - Log Scale\")\n",
    "plt.ylabel(\"Time (seconds) - Log Scale\")\n",
    "plt.title(\"UnionFind Performance (Log-Log Scale)\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=GRID_ALPHA)\n",
    "\n",
    "# Subplot 3: 3Sum performance with fitted curves\n",
    "plt.subplot(2, 2, 3)\n",
    "for algorithm in threesum_df[\"Algorithm\"].unique():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm]\n",
    "    style = threesum_styles[algorithm]\n",
    "    plt.plot(\n",
    "        data[\"Array Size\"], data[\"Time (s)\"], label=f\"{algorithm} (data)\",\n",
    "        marker=style[\"marker\"], linestyle=style[\"linestyle\"],\n",
    "        markersize=style[\"markersize\"], linewidth=LINE_WIDTH\n",
    "    )\n",
    "\n",
    "    # Add fitted curve overlay with appropriate range\n",
    "    if algorithm in fitted_params:\n",
    "        a, b = fitted_params[algorithm][:FIT_TYPE_INDEX]\n",
    "        fit_type = (\n",
    "            fitted_params[algorithm][FIT_TYPE_INDEX]\n",
    "            if len(fitted_params[algorithm]) > FIT_TYPE_INDEX\n",
    "            else 'simple'\n",
    "        )\n",
    "\n",
    "        if fit_type == 'large_only' and \"Two Pointers\" in algorithm:\n",
    "            # For Two Pointers large-only fit, show curve only for larger sizes\n",
    "            large_data = data.tail(LARGE_SIZE_TAIL)\n",
    "            x_fit = np.linspace(\n",
    "                large_data[\"Array Size\"].min(),\n",
    "                data[\"Array Size\"].max(),\n",
    "                PARTIAL_FIT_POINTS\n",
    "            )\n",
    "            fit_label = f\"{algorithm} (fit: N^{b:.1f}, large sizes)\"\n",
    "        else:\n",
    "            # Standard full-range fitting\n",
    "            x_fit = np.linspace(\n",
    "                data[\"Array Size\"].min(),\n",
    "                data[\"Array Size\"].max(),\n",
    "                FIT_POINTS\n",
    "            )\n",
    "            fit_label = f\"{algorithm} (fit: N^{b:.1f})\"\n",
    "\n",
    "        y_fit = power_law(x_fit, a, b)\n",
    "        plt.plot(\n",
    "            x_fit, y_fit, \"--\", alpha=FIT_ALPHA,\n",
    "            linewidth=FIT_LINE_WIDTH, label=fit_label\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Array Size\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"3Sum Performance with Fitted Curves\")\n",
    "plt.legend(fontsize=7, loc='upper left')\n",
    "plt.grid(True, alpha=GRID_ALPHA)\n",
    "\n",
    "# Subplot 4: 3Sum performance on log-log scale with fitted curves\n",
    "plt.subplot(2, 2, 4)\n",
    "for algorithm in threesum_df[\"Algorithm\"].unique():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm]\n",
    "    style = threesum_styles[algorithm]\n",
    "    plt.loglog(\n",
    "        data[\"Array Size\"], data[\"Time (s)\"], label=f\"{algorithm} (data)\",\n",
    "        marker=style[\"marker\"], linestyle=style[\"linestyle\"],\n",
    "        markersize=style[\"markersize\"], linewidth=LINE_WIDTH\n",
    "    )\n",
    "\n",
    "    # Add fitted curve overlay with appropriate range\n",
    "    if algorithm in fitted_params:\n",
    "        a, b = fitted_params[algorithm][:FIT_TYPE_INDEX]\n",
    "        fit_type = (\n",
    "            fitted_params[algorithm][FIT_TYPE_INDEX]\n",
    "            if len(fitted_params[algorithm]) > FIT_TYPE_INDEX\n",
    "            else 'simple'\n",
    "        )\n",
    "\n",
    "        if fit_type == 'large_only' and \"Two Pointers\" in algorithm:\n",
    "            # For Two Pointers large-only fit, show curve only for larger sizes\n",
    "            large_data = data.tail(LARGE_SIZE_TAIL)\n",
    "            x_fit = np.linspace(\n",
    "                large_data[\"Array Size\"].min(),\n",
    "                data[\"Array Size\"].max(),\n",
    "                PARTIAL_FIT_POINTS\n",
    "            )\n",
    "            fit_label = f\"{algorithm} (fit: N^{b:.1f}, large sizes)\"\n",
    "        else:\n",
    "            # Standard full-range fitting\n",
    "            x_fit = np.linspace(\n",
    "                data[\"Array Size\"].min(),\n",
    "                data[\"Array Size\"].max(),\n",
    "                FIT_POINTS\n",
    "            )\n",
    "            fit_label = f\"{algorithm} (fit: N^{b:.1f})\"\n",
    "\n",
    "        y_fit = power_law(x_fit, a, b)\n",
    "        plt.loglog(\n",
    "            x_fit, y_fit, \"--\", alpha=FIT_ALPHA,\n",
    "            linewidth=FIT_LINE_WIDTH, label=fit_label\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Array Size - Log Scale\")\n",
    "plt.ylabel(\"Time (seconds) - Log Scale\")\n",
    "plt.title(\"3Sum Performance (Log-Log) with Fitted Curves\")\n",
    "plt.legend(fontsize=7, loc='upper left')\n",
    "plt.grid(True, alpha=GRID_ALPHA)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Enhanced visualizations with improved curve fitting created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
