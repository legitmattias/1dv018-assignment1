{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Algorithms and Data Structures Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs empirical analysis of algorithm performance to verify theoretical Big O complexity predictions. We test two fundamental algorithmic problems with multiple implementations to demonstrate how different approaches scale with input size.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook tests and visualizes the performance of:\n",
    "\n",
    "**UnionFind Algorithms** (4 implementations):\n",
    "- **Quick Find**: O(1) find, O(N) union - simple but inefficient for many unions\n",
    "- **Quick Union**: O(N) find, O(1) union - better for sparse operations\n",
    "- **Weighted Quick Union**: O(log N) find, O(log N) union - balanced approach with tree size tracking\n",
    "- **Weighted Quick Union with Path Compression**: O(α(N)) amortized - optimal performance\n",
    "- Tested with varying input sizes (1K-100K elements) and proportional operations\n",
    "\n",
    "**3Sum Algorithms** (3 implementations):\n",
    "- **Brute Force**: O(N³) - checks all possible triplets\n",
    "- **Optimized Two Pointers**: O(N²) - sorts array and uses two-pointer technique\n",
    "- **Hash Set**: O(N²) - uses hash table for constant-time lookups\n",
    "- Tested with different array sizes (80-8K elements)\n",
    "\n",
    "**Analysis Methodology**:\n",
    "- Performance plots comparing execution times across input sizes\n",
    "- Log-log scale plots to verify Big O complexity slopes (slope = complexity exponent)\n",
    "- Statistical timing measurements using %timeit for accuracy\n",
    "- Complexity verification through slope analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add src directory to path to import custom algorithm implementations\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Import custom algorithm implementations\n",
    "from threesum import (\n",
    "    generate_test_data,\n",
    "    three_sum_brute_force,\n",
    "    three_sum_optimized,\n",
    "    three_sum_optimized_with_hash,\n",
    ")\n",
    "from unionfind import (\n",
    "    QuickFind,\n",
    "    QuickUnion,\n",
    "    WeightedQuickUnion,\n",
    "    WeightedQuickUnionPathCompression,\n",
    ")\n",
    "\n",
    "# Configure plotting style for pretty-looking graphs\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducible results across runs for consistency\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for performance testing and data generation\n",
    "\n",
    "\n",
    "def setup_unionfind_test(uf_class, n: int, operations: list[tuple[int, int]]):\n",
    "    \"\"\"\n",
    "    Setup UnionFind instance and operations for %timeit testing.\n",
    "\n",
    "    This function creates a closure that contains the UnionFind instance\n",
    "    and all operations to be performed, allowing %timeit to measure\n",
    "    just the algorithm execution time without setup overhead.\n",
    "    \"\"\"\n",
    "    uf = uf_class(n)\n",
    "\n",
    "    def run_operations():\n",
    "        for p, q in operations:\n",
    "            uf.union(p, q)\n",
    "\n",
    "    return run_operations\n",
    "\n",
    "\n",
    "def generate_unionfind_operations(n: int, num_operations: int) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Generate random union operations for testing.\n",
    "\n",
    "    Creates a list of random (p, q) pairs where both p and q are\n",
    "    valid indices in the range [0, n-1]. The number of operations\n",
    "    is typically proportional to n to ensure meaningful performance\n",
    "    comparisons across different input sizes.\n",
    "    \"\"\"\n",
    "    operations = []\n",
    "    for _ in range(num_operations):\n",
    "        p = random.randint(0, n - 1)\n",
    "        q = random.randint(0, n - 1)\n",
    "        operations.append((p, q))\n",
    "    return operations\n",
    "\n",
    "\n",
    "def setup_threesum_test(func, nums: list[int]):\n",
    "    \"\"\"\n",
    "    Setup 3Sum function for %timeit testing.\n",
    "\n",
    "    Creates a closure that contains the test data and function\n",
    "    to be tested, allowing %timeit to measure just the algorithm\n",
    "    execution time without data generation overhead.\n",
    "\n",
    "    Uses value-based deduplication to avoid inflated result counts.\n",
    "    \"\"\"\n",
    "\n",
    "    def run_threesum():\n",
    "        return func(nums, return_values=True)\n",
    "\n",
    "    return run_threesum\n",
    "\n",
    "\n",
    "print(\"Performance measurement functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UnionFind Performance Analysis\n",
    "# =============================================================================\n",
    "# This section tests the performance of four different UnionFind implementations\n",
    "# across varying input sizes to verify their theoretical time complexities.\n",
    "\n",
    "print(\"=== UnionFind Performance Analysis ===\")\n",
    "\n",
    "# Test parameters: Input sizes from 1K to 100K elements\n",
    "# These sizes are chosen to show clear performance differences between algorithms\n",
    "# and allow for meaningful slope analysis in log-log plots\n",
    "n_values = [1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "# UnionFind algorithms to test - ordered from least to most efficient\n",
    "uf_algorithms = [\n",
    "    (\"Quick Find\", QuickFind),  # O(1) find, O(N) union\n",
    "    (\"Quick Union\", QuickUnion),  # O(N) find, O(1) union\n",
    "    (\"Weighted Quick Union\", WeightedQuickUnion),  # O(log N) find, O(log N) union\n",
    "    (\n",
    "        \"Weighted Quick Union with Path Compression\",\n",
    "        WeightedQuickUnionPathCompression,\n",
    "    ),  # O(α(N)) amortized\n",
    "]\n",
    "\n",
    "# Store performance results for analysis and visualization\n",
    "uf_results = []\n",
    "\n",
    "# Test each algorithm with each input size\n",
    "for n in n_values:\n",
    "    # Scale operations proportionally to N\n",
    "    # 0.9 ratio ensures sufficient operations while avoiding complete connectivity\n",
    "    # This creates a realistic workload where most elements get connected\n",
    "    num_operations = int(0.9 * n)\n",
    "    print(f\"\\nTesting with N = {n}, Operations = {num_operations}\")\n",
    "    operations = generate_unionfind_operations(n, num_operations)\n",
    "\n",
    "    # Test each UnionFind implementation with the same set of operations\n",
    "    for name, uf_class in uf_algorithms:\n",
    "        test_func = setup_unionfind_test(uf_class, n, operations)\n",
    "\n",
    "        # Use %timeit for accurate timing measurements\n",
    "        # -q flag suppresses output, -o flag returns timing object\n",
    "        result = %timeit -q -o test_func() # pyright: ignore\n",
    "\n",
    "        # Store results for later analysis\n",
    "        uf_results.append(\n",
    "            {\n",
    "                \"Algorithm\": name,\n",
    "                \"N\": n,\n",
    "                \"Operations\": num_operations,\n",
    "                \"Time (s)\": result.best,  # Best time from multiple runs\n",
    "                \"Average (s)\": result.average,  # Average time from multiple runs\n",
    "            }\n",
    "        )\n",
    "        print(f\"  {name}: {result.best:.6f}s (best), {result.average:.6f}s (avg)\")\n",
    "\n",
    "print(f\"\\nUnionFind analysis completed! {len(uf_results)} measurements taken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3Sum Performance Analysis\n",
    "# =============================================================================\n",
    "# This section tests three different approaches to the 3Sum problem:\n",
    "# 1. Brute Force: O(N³) - checks all possible triplets\n",
    "# 2. Optimized Two Pointers: O(N²) - sorts array and uses two-pointer technique\n",
    "# 3. Hash Set: O(N²) - uses hash table for constant-time lookups\n",
    "#\n",
    "# NOTE: All algorithms now use value-based deduplication (return_values=True)\n",
    "# to avoid inflated result counts from duplicate values in random test data.\n",
    "\n",
    "print(\"=== 3Sum Performance Analysis ===\")\n",
    "\n",
    "# Test parameters for 3Sum algorithms\n",
    "# Different array sizes are used for different algorithms based on their complexity:\n",
    "# - Brute force: smaller sizes (80-400) due to O(N³) complexity\n",
    "# - Optimized algorithms: larger sizes (500-8000) due to O(N²) complexity\n",
    "array_sizes_brute = [80, 120, 200, 300, 400]  # Smaller sizes for O(N³) algorithm\n",
    "array_sizes_optimized = [\n",
    "    500,\n",
    "    1000,\n",
    "    2000,\n",
    "    5000,\n",
    "    8000,\n",
    "]  # Larger sizes for O(N²) algorithms\n",
    "\n",
    "# 3Sum algorithms to test - ordered by theoretical efficiency\n",
    "threesum_algorithms = [\n",
    "    (\"Brute Force\", three_sum_brute_force),  # O(N³) - checks all triplets\n",
    "    (\"Optimized Two Pointers\", three_sum_optimized),  # O(N²) - two-pointer technique\n",
    "    (\"Hash Set\", three_sum_optimized_with_hash),  # O(N²) - hash table approach\n",
    "]\n",
    "\n",
    "# Store performance results for analysis and visualization\n",
    "threesum_results = []\n",
    "\n",
    "# Test brute force algorithm with smaller array sizes\n",
    "# Due to O(N³) complexity, we use smaller sizes to keep execution time reasonable\n",
    "print(\"\\n--- Testing Brute Force with smaller sizes ---\")\n",
    "for size in array_sizes_brute:\n",
    "    print(f\"\\nTesting Brute Force with array size = {size}\")\n",
    "\n",
    "    # Generate random test data for this array size\n",
    "    test_data = generate_test_data(size)\n",
    "\n",
    "    name, func = threesum_algorithms[0]  # Brute Force algorithm\n",
    "    test_func = setup_threesum_test(func, test_data)\n",
    "\n",
    "    # Run algorithm once to get solution count and verify correctness\n",
    "    sample_result = func(test_data, return_values=True)\n",
    "    result = %timeit -q -o test_func()  # pyright: ignore\n",
    "\n",
    "    # Store results including number of solutions found\n",
    "    threesum_results.append(\n",
    "        {\n",
    "            \"Algorithm\": name,\n",
    "            \"Array Size\": size,\n",
    "            \"Time (s)\": result.best,\n",
    "            \"Average (s)\": result.average,\n",
    "            \"Solutions Found\": len(sample_result),\n",
    "        }\n",
    "    )\n",
    "    print(\n",
    "        f\"  {name}: {result.best:.6f}s (best), \"\n",
    "        f\"{result.average:.6f}s (avg), {len(sample_result)} unique value triplets\"\n",
    "    )\n",
    "\n",
    "# Test optimized algorithms with larger array sizes\n",
    "# Due to O(N²) complexity, test with larger sizes and keeping execution time reasonable\n",
    "print(\"\\n--- Testing Optimized algorithms with larger sizes ---\")\n",
    "for size in array_sizes_optimized:\n",
    "    print(f\"\\nTesting with array size = {size}\")\n",
    "\n",
    "    # Generate random test data for this array size\n",
    "    test_data = generate_test_data(size)\n",
    "\n",
    "    # Test both optimized algorithms (skip brute force)\n",
    "    for name, func in threesum_algorithms[1:]:  # Skip brute force\n",
    "        test_func = setup_threesum_test(func, test_data)\n",
    "\n",
    "        # Run algorithm once to get solution count and verify correctness\n",
    "        sample_result = func(test_data, return_values=True)\n",
    "        result = %timeit -q -o test_func()  # pyright: ignore\n",
    "\n",
    "        # Store results including number of solutions found\n",
    "        threesum_results.append(\n",
    "            {\n",
    "                \"Algorithm\": name,\n",
    "                \"Array Size\": size,\n",
    "                \"Time (s)\": result.best,\n",
    "                \"Average (s)\": result.average,\n",
    "                \"Solutions Found\": len(sample_result),\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"  {name}: {result.best:.6f}s (best), \"\n",
    "            f\"{result.average:.6f}s (avg), {len(sample_result)} unique value triplets\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n3Sum analysis completed! {len(threesum_results)} measurements taken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Performance Visualization and Complexity Analysis\n",
    "# =============================================================================\n",
    "# This section creates visualizations and performs complexity analysis to verify\n",
    "# that the empirical results match the theoretical Big O predictions.\n",
    "\n",
    "print(\"=== Creating Performance Visualizations ===\")\n",
    "\n",
    "# Convert results to DataFrames for easier plotting and analysis\n",
    "uf_df = pd.DataFrame(uf_results)\n",
    "threesum_df = pd.DataFrame(threesum_results)\n",
    "\n",
    "# Slope verification for complexity analysis\n",
    "# In log-log plots, the slope of the line represents the exponent in the complexity\n",
    "# For example: O(N²) has slope ≈ 2, O(N³) has slope ≈ 3, O(log N) has slope ≈ 0.3\n",
    "print(\"\\n=== Slope Analysis for Complexity Verification ===\")\n",
    "\n",
    "# Minimum data points required for meaningful slope calculation\n",
    "MIN_DATA_POINTS = 2\n",
    "\n",
    "# UnionFind slope analysis\n",
    "# Expected slopes: Quick Find ≈ 1 (O(N) union), Quick Union ≈ 1 (O(N) find),\n",
    "# Weighted Quick Union ≈ 0.3 (O(log N)), Path Compression ≈ 0.1 (O(α(N)))\n",
    "print(\"\\nUnionFind Complexity Verification:\")\n",
    "for algorithm in uf_df[\"Algorithm\"].unique():\n",
    "    data = uf_df[uf_df[\"Algorithm\"] == algorithm].sort_values(\"N\")\n",
    "    if len(data) >= MIN_DATA_POINTS:\n",
    "        # Calculate slope between first and last points in log-log space\n",
    "        # This gives us the exponent of the complexity function\n",
    "        log_n = np.log10(data[\"N\"].values)\n",
    "        log_time = np.log10(data[\"Time (s)\"].values)\n",
    "        slope = (log_time[-1] - log_time[0]) / (log_n[-1] - log_n[0])\n",
    "        print(f\"  {algorithm}: slope = {slope:.2f}\")\n",
    "\n",
    "# 3Sum slope analysis\n",
    "# Expected slopes: Brute Force ≈ 3 (O(N³)), Optimized algorithms ≈ 2 (O(N²))\n",
    "print(\"\\n3Sum Complexity Verification:\")\n",
    "for algorithm in threesum_df[\"Algorithm\"].unique():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm].sort_values(\"Array Size\")\n",
    "    if len(data) >= MIN_DATA_POINTS:\n",
    "        # Calculate slope in log-log space to determine complexity exponent\n",
    "        log_size = np.log10(data[\"Array Size\"].values)\n",
    "        log_time = np.log10(data[\"Time (s)\"].values)\n",
    "        slope = (log_time[-1] - log_time[0]) / (log_size[-1] - log_size[0])\n",
    "        print(\n",
    "            f\"  {algorithm}: slope = {slope:.2f} \"\n",
    "            f\"(expected: ~2.0 for O(N²), ~3.0 for O(N³))\"\n",
    "        )\n",
    "\n",
    "# Create comprehensive performance visualization with 4 subplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Subplot 1: UnionFind performance on linear scale\n",
    "# Shows absolute performance differences between algorithms\n",
    "plt.subplot(2, 2, 1)\n",
    "for algorithm in uf_df[\"Algorithm\"].unique():\n",
    "    data = uf_df[uf_df[\"Algorithm\"] == algorithm]\n",
    "    plt.plot(data[\"N\"], data[\"Time (s)\"], marker=\"o\", label=algorithm)\n",
    "plt.xlabel(\"Number of Elements (N)\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"UnionFind Performance Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: UnionFind performance on log-log scale\n",
    "# Log-log plots reveal the complexity by showing straight lines with slopes\n",
    "# equal to the complexity exponent (e.g., slope 1 = O(N), slope 2 = O(N²))\n",
    "plt.subplot(2, 2, 2)\n",
    "for algorithm in uf_df[\"Algorithm\"].unique():\n",
    "    data = uf_df[uf_df[\"Algorithm\"] == algorithm]\n",
    "    plt.loglog(data[\"N\"], data[\"Time (s)\"], marker=\"o\", label=algorithm)\n",
    "plt.xlabel(\"Number of Elements (N) - Log Scale\")\n",
    "plt.ylabel(\"Time (seconds) - Log Scale\")\n",
    "plt.title(\"UnionFind Performance (Log-Log Scale)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: 3Sum performance on linear scale\n",
    "# Shows how the O(N³) brute force algorithm becomes impractical for larger inputs\n",
    "# while the O(N²) optimized algorithms scale much better\n",
    "plt.subplot(2, 2, 3)\n",
    "for algorithm in threesum_df[\"Algorithm\"].unique():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm]\n",
    "    plt.plot(data[\"Array Size\"], data[\"Time (s)\"], marker=\"s\", label=algorithm)\n",
    "plt.xlabel(\"Array Size\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"3Sum Performance Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: 3Sum performance on log-log scale\n",
    "# The slope of the lines reveals the complexity: Brute Force should have slope ≈ 3,\n",
    "# while optimized algorithms should have slope ≈ 2\n",
    "plt.subplot(2, 2, 4)\n",
    "for algorithm in threesum_df[\"Algorithm\"].unique():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm]\n",
    "    plt.loglog(data[\"Array Size\"], data[\"Time (s)\"], marker=\"s\", label=algorithm)\n",
    "plt.xlabel(\"Array Size - Log Scale\")\n",
    "plt.ylabel(\"Time (seconds) - Log Scale\")\n",
    "plt.title(\"3Sum Performance (Log-Log Scale)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Adjust layout to prevent overlapping subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
