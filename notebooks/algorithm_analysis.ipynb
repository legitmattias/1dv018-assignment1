{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Algorithms and Data Structures Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook benchmarks Union–Find (four variants) and 3Sum (three variants) using `%timeit` with closure-based harnesses, plots time vs. input size on log–log scales, and applies power-law curve fitting to 3Sum (with an optional large-N fit for Two Pointers). \n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook tests and visualizes the performance of:\n",
    "\n",
    "**UnionFind Algorithms** (4 implementations):\n",
    "- **Quick Find**: O(1) connected, O(N) union - simple but inefficient for many unions\n",
    "- **Quick Union**: O(N) connected, O(1) union - better for sparse operations\n",
    "- **Weighted Quick Union**: O(log N) connected, O(log N) union - balanced approach with tree size tracking\n",
    "- **Weighted Quick Union with Path Compression**: O(α(N)) amortized - optimal performance\n",
    "- Tested with varying input sizes (1K-100K elements) and proportional operations\n",
    "\n",
    "**3Sum Algorithms** (3 implementations):\n",
    "- **Brute Force**: O(N³) - checks all possible triplets\n",
    "- **Optimized Two Pointers**: O(N²) - sorts array and uses two-pointer technique\n",
    "- **Hash Set**: O(N²) - uses hash table for constant-time lookups\n",
    "- Tested with different array sizes (80-8K elements)\n",
    "\n",
    "**Analysis Methodology**:\n",
    "- Performance plots comparing execution times across input sizes\n",
    "- Log-log scale plots to verify Big O complexity slopes (slope = complexity exponent)\n",
    "- Statistical timing measurements using %timeit for accuracy\n",
    "- Complexity verification through slope analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add src directory to path to import custom algorithm implementations\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Import custom algorithm implementations\n",
    "from threesum import (\n",
    "    generate_test_data,\n",
    "    three_sum_brute_force,\n",
    "    three_sum_optimized,\n",
    "    three_sum_optimized_with_hash,\n",
    ")\n",
    "from unionfind import (\n",
    "    QuickFind,\n",
    "    QuickUnion,\n",
    "    WeightedQuickUnion,\n",
    "    WeightedQuickUnionPathCompression,\n",
    ")\n",
    "\n",
    "# Configure plotting style for pretty-looking graphs\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducible results across runs for consistency\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for performance testing and data generation\n",
    "\n",
    "\n",
    "def setup_unionfind_test(uf_class, n: int, operations: list[tuple[int, int]]):\n",
    "    \"\"\"\n",
    "    Setup UnionFind instance and operations for %timeit testing.\n",
    "\n",
    "    This function creates a closure that contains the UnionFind instance\n",
    "    and all operations to be performed, allowing %timeit to measure\n",
    "    just the algorithm execution time without setup overhead.\n",
    "    \"\"\"\n",
    "    uf = uf_class(n)\n",
    "\n",
    "    def run_operations():\n",
    "        for p, q in operations:\n",
    "            uf.union(p, q)\n",
    "\n",
    "    return run_operations\n",
    "\n",
    "\n",
    "def setup_unionfind_connected_test(\n",
    "    uf_class, n: int, union_ops: list[tuple[int, int]], connected_ops: list[int]\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup UnionFind instance with pre-existing unions for connected operation testing.\n",
    "\n",
    "    This function creates a UnionFind structure with some connections already\n",
    "    established, then measures the performance of connected operations.\n",
    "    \"\"\"\n",
    "    uf = uf_class(n)\n",
    "\n",
    "    # Pre-populate with union operations to create a realistic structure\n",
    "    for p, q in union_ops:\n",
    "        uf.union(p, q)\n",
    "\n",
    "    def run_connected_operations():\n",
    "        for element in connected_ops:\n",
    "            uf.find(element)\n",
    "\n",
    "    return run_connected_operations\n",
    "\n",
    "\n",
    "def generate_unionfind_operations(n: int, num_operations: int) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Generate random union operations for testing.\n",
    "\n",
    "    Creates a list of random (p, q) pairs where both p and q are\n",
    "    valid indices in the range [0, n-1]. The number of operations\n",
    "    is typically proportional to n to ensure meaningful performance\n",
    "    comparisons across different input sizes.\n",
    "    \"\"\"\n",
    "    operations = []\n",
    "    for _ in range(num_operations):\n",
    "        p = random.randint(0, n - 1)\n",
    "        q = random.randint(0, n - 1)\n",
    "        operations.append((p, q))\n",
    "    return operations\n",
    "\n",
    "\n",
    "def generate_connected_operations(n: int, num_operations: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Generate random connected operations for testing.\n",
    "\n",
    "    Creates a list of random element indices for connected operations.\n",
    "    These represent realistic queries about which component an element belongs to.\n",
    "    \"\"\"\n",
    "    operations = []\n",
    "    for _ in range(num_operations):\n",
    "        element = random.randint(0, n - 1)\n",
    "        operations.append(element)\n",
    "    return operations\n",
    "\n",
    "\n",
    "def setup_threesum_test(func, nums: list[int]):\n",
    "    \"\"\"\n",
    "    Setup 3Sum function for %timeit testing.\n",
    "\n",
    "    Creates a closure that contains the test data and function\n",
    "    to be tested, allowing %timeit to measure just the algorithm\n",
    "    execution time without data generation overhead.\n",
    "\n",
    "    Uses value-based deduplication to avoid inflated result counts.\n",
    "    \"\"\"\n",
    "\n",
    "    def run_threesum():\n",
    "        return func(nums, return_values=True)\n",
    "\n",
    "    return run_threesum\n",
    "\n",
    "\n",
    "print(\"Performance measurement functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UnionFind Performance Analysis - Union and Connected Operations\n",
    "# =============================================================================\n",
    "# This section tests the performance of four different UnionFind implementations\n",
    "# across varying input sizes to verify their theoretical time complexities.\n",
    "# It measures UNION and CONNECTED operations separately.\n",
    "\n",
    "print(\"=== UnionFind Performance Analysis ===\")\n",
    "\n",
    "# Test parameters: Input sizes from 1K to 100K elements\n",
    "# These sizes are chosen to show clear performance differences between algorithms\n",
    "# and allow for meaningful slope analysis in log-log plots\n",
    "n_values = [1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "# UnionFind algorithms to test - ordered from least to most efficient\n",
    "uf_algorithms = [\n",
    "    (\"Quick Find\", QuickFind),  # O(1) connected, O(N) union\n",
    "    (\"Quick Union\", QuickUnion),  # O(N) connected, O(1) union\n",
    "    (\"Weighted Quick Union\", WeightedQuickUnion),  # O(log N) connected, O(log N) union\n",
    "    (\n",
    "        \"Weighted Quick Union with Path Compression\",\n",
    "        WeightedQuickUnionPathCompression,\n",
    "    ),  # O(α(N)) amortized\n",
    "]\n",
    "\n",
    "# Store performance results for analysis and visualization\n",
    "uf_union_results = []\n",
    "uf_connected_results = []\n",
    "\n",
    "# Test each algorithm with each input size\n",
    "for n in n_values:\n",
    "    # Scale operations proportionally to N\n",
    "    # 0.9 ratio ensures sufficient operations while avoiding complete connectivity\n",
    "    # This creates a realistic workload where most elements get connected\n",
    "    num_operations = int(0.9 * n)\n",
    "    print(f\"\\nTesting with N = {n}, Operations = {num_operations}\")\n",
    "\n",
    "    # Generate consistent operations for both union and connected tests\n",
    "    union_operations = generate_unionfind_operations(n, num_operations)\n",
    "    connected_operations = generate_connected_operations(n, num_operations)\n",
    "\n",
    "    # Test each UnionFind implementation with the same set of operations\n",
    "    for name, uf_class in uf_algorithms:\n",
    "        # UNION operation testing\n",
    "        union_test_func = setup_unionfind_test(uf_class, n, union_operations)\n",
    "        union_result = %timeit -q -o union_test_func() # pyright: ignore\n",
    "\n",
    "        # Store union results\n",
    "        uf_union_results.append(\n",
    "            {\n",
    "                \"Algorithm\": name,\n",
    "                \"Operation\": \"Union\",\n",
    "                \"N\": n,\n",
    "                \"Operations\": num_operations,\n",
    "                \"Time (s)\": union_result.best,\n",
    "                \"Average (s)\": union_result.average,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # CONNECTED operation testing (with pre-established structure)\n",
    "        connected_test_func = setup_unionfind_connected_test(\n",
    "            uf_class, n, union_operations, connected_operations\n",
    "        )\n",
    "        connected_result = %timeit -q -o connected_test_func() # pyright: ignore\n",
    "\n",
    "        # Store connected results\n",
    "        uf_connected_results.append(\n",
    "            {\n",
    "                \"Algorithm\": name,\n",
    "                \"Operation\": \"Connected\",\n",
    "                \"N\": n,\n",
    "                \"Operations\": num_operations,\n",
    "                \"Time (s)\": connected_result.best,\n",
    "                \"Average (s)\": connected_result.average,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"  {name}:\")\n",
    "        print(\n",
    "            f\"    Union:     {union_result.best:.6f}s (best), \"\n",
    "            f\"{union_result.average:.6f}s (avg)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"    Connected: {connected_result.best:.6f}s (best), \"\n",
    "            f\"{connected_result.average:.6f}s (avg)\"\n",
    "        )\n",
    "\n",
    "# Combine results for easier analysis\n",
    "uf_results = uf_union_results + uf_connected_results\n",
    "\n",
    "print(f\"\\nUnionFind analysis completed! {len(uf_results)} measurements taken.\")\n",
    "print(f\"Union measurements: {len(uf_union_results)}\")\n",
    "print(f\"Connected measurements: {len(uf_connected_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3Sum Performance Analysis\n",
    "# =============================================================================\n",
    "# This section tests three different approaches to the 3Sum problem:\n",
    "# 1. Brute Force: O(N³) - checks all possible triplets\n",
    "# 2. Optimized Two Pointers: O(N²) - sorts array and uses two-pointer technique\n",
    "# 3. Hash Set: O(N²) - uses hash table for constant-time lookups\n",
    "#\n",
    "# NOTE: All algorithms use value-based deduplication (return_values=True)\n",
    "# to avoid inflated result counts from duplicate values in random test data.\n",
    "# The measurements include the complete \"find\" operation including any setup\n",
    "# like sorting, as this is part of the overall algorithmic solution.\n",
    "\n",
    "print(\"=== 3Sum Performance Analysis ===\")\n",
    "\n",
    "# Test parameters for 3Sum algorithms\n",
    "# Different array sizes are used for different algorithms based on their complexity:\n",
    "# - Brute force: smaller sizes (80-400) due to O(N³) complexity\n",
    "# - Optimized algorithms: larger sizes (500-8000) due to O(N²) complexity\n",
    "array_sizes_brute = [80, 120, 200, 300, 400]  # Smaller sizes for O(N³) algorithm\n",
    "array_sizes_optimized = [\n",
    "    500,\n",
    "    1000,\n",
    "    2000,\n",
    "    5000,\n",
    "    8000,\n",
    "]  # Larger sizes for O(N²) algorithms\n",
    "\n",
    "# 3Sum algorithms to test - ordered by theoretical efficiency\n",
    "threesum_algorithms = [\n",
    "    (\"Brute Force\", three_sum_brute_force),  # O(N³) - checks all triplets\n",
    "    (\"Optimized Two Pointers\", three_sum_optimized),  # O(N²) - two-pointer technique\n",
    "    (\"Hash Set\", three_sum_optimized_with_hash),  # O(N²) - hash table approach\n",
    "]\n",
    "\n",
    "# Store performance results for analysis and visualization\n",
    "threesum_results = []\n",
    "\n",
    "# Test brute force algorithm with smaller array sizes\n",
    "# Due to O(N³) complexity, we use smaller sizes to keep execution time reasonable\n",
    "print(\"\\n--- Testing Brute Force Find Operations with smaller sizes ---\")\n",
    "for size in array_sizes_brute:\n",
    "    print(f\"\\nTesting Brute Force find triplets with array size = {size}\")\n",
    "\n",
    "    # Generate random test data for this array size\n",
    "    test_data = generate_test_data(size)\n",
    "\n",
    "    name, func = threesum_algorithms[0]  # Brute Force algorithm\n",
    "    test_func = setup_threesum_test(func, test_data)\n",
    "\n",
    "    # Run algorithm once to get solution count and verify correctness\n",
    "    sample_result = func(test_data, return_values=True)\n",
    "    result = %timeit -q -o test_func()  # pyright: ignore\n",
    "\n",
    "    # Store results including number of solutions found\n",
    "    threesum_results.append(\n",
    "        {\n",
    "            \"Algorithm\": name,\n",
    "            \"Operation\": \"Find\",\n",
    "            \"Array Size\": size,\n",
    "            \"Time (s)\": result.best,\n",
    "            \"Average (s)\": result.average,\n",
    "            \"Solutions Found\": len(sample_result),\n",
    "        }\n",
    "    )\n",
    "    print(\n",
    "        f\"  {name}: {result.best:.6f}s (best), \"\n",
    "        f\"{result.average:.6f}s (avg), {len(sample_result)} unique value triplets\"\n",
    "    )\n",
    "\n",
    "# Test optimized algorithms with larger array sizes\n",
    "print(\"\\n--- Testing Optimized Find Operations with larger sizes ---\")\n",
    "for size in array_sizes_optimized:\n",
    "    print(f\"\\nTesting find triplets with array size = {size}\")\n",
    "\n",
    "    # Generate random test data for this array size\n",
    "    test_data = generate_test_data(size)\n",
    "\n",
    "    # Test both optimized algorithms (skip brute force)\n",
    "    for name, func in threesum_algorithms[1:]:  # Skip brute force\n",
    "        test_func = setup_threesum_test(func, test_data)\n",
    "\n",
    "        # Run algorithm once to get solution count and verify correctness\n",
    "        sample_result = func(test_data, return_values=True)\n",
    "        result = %timeit -q -o test_func()  # pyright: ignore\n",
    "\n",
    "        # Store results including number of solutions found\n",
    "        threesum_results.append(\n",
    "            {\n",
    "                \"Algorithm\": name,\n",
    "                \"Operation\": \"Find\",\n",
    "                \"Array Size\": size,\n",
    "                \"Time (s)\": result.best,\n",
    "                \"Average (s)\": result.average,\n",
    "                \"Solutions Found\": len(sample_result),\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"  {name}: {result.best:.6f}s (best), \"\n",
    "            f\"{result.average:.6f}s (avg), {len(sample_result)} unique value triplets\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"\\n3Sum find triplets analysis completed! \"\n",
    "    f\"{len(threesum_results)} measurements taken.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Power-Law Curve Fitting - Simple Constants Extraction\n",
    "# =============================================================================\n",
    "\n",
    "# Convert results to DataFrames for easier analysis\n",
    "threesum_df = pd.DataFrame(threesum_results)\n",
    "\n",
    "def power_law(x, a, b):\n",
    "    return a * (x**b)\n",
    "\n",
    "def power_law_with_offset(x, a, b, c):\n",
    "    \"\"\"Power law with constant offset: c + a * x^b\"\"\"\n",
    "    return c + a * (x**b)\n",
    "\n",
    "print(\"=== Power-Law Constants: time ≈ a·N^b ===\\n\")\n",
    "\n",
    "# Store fitting results for overlay plotting\n",
    "fitted_params = {}\n",
    "\n",
    "MIN_FITTING_POINTS = 2  # Minimum data points required for curve fitting\n",
    "MIN_LARGE_POINTS = 3  # Minimum points for large-size fitting strategy\n",
    "\n",
    "for algorithm in threesum_df[\"Algorithm\"].unique():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm].sort_values(\"Array Size\")\n",
    "    print(f\"\\nFitting {algorithm}:\")\n",
    "    print(f\"  Data points: {len(data)}\")\n",
    "    print(f\"  Size range: {data['Array Size'].min()}-{data['Array Size'].max()}\")\n",
    "    print(f\"  Time range: {data['Time (s)'].min():.6f}-{data['Time (s)'].max():.6f}\")\n",
    "\n",
    "    # Show actual data points to understand the trend\n",
    "    for _, row in data.iterrows():\n",
    "        print(f\"    Size {row['Array Size']:4d}: {row['Time (s)']:.6f}s\")\n",
    "\n",
    "    if len(data) >= MIN_FITTING_POINTS:\n",
    "        # Try simple power law first\n",
    "        try:\n",
    "            (a, b), _ = curve_fit(power_law, data[\"Array Size\"], data[\"Time (s)\"])\n",
    "            simple_fit = (a, b)\n",
    "            print(f\"  Simple fit: a={a:.2e}, b={b:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Simple fitting failed: {e}\")\n",
    "            simple_fit = None\n",
    "\n",
    "        # For Two Pointers, try fitting only larger sizes where scaling is clear\n",
    "        if \"Two Pointers\" in algorithm and len(data) >= MIN_LARGE_POINTS:\n",
    "            try:\n",
    "                # Use only the largest 3 data points where true scaling emerges\n",
    "                large_data = data.tail(MIN_LARGE_POINTS)\n",
    "                print(\n",
    "                    f\"  Trying fit on large sizes only: \"\n",
    "                    f\"{large_data['Array Size'].tolist()}\"\n",
    "                )\n",
    "                (a_large, b_large), _ = curve_fit(\n",
    "                    power_law, large_data[\"Array Size\"], large_data[\"Time (s)\"]\n",
    "                )\n",
    "                print(f\"  Large-size fit: a={a_large:.2e}, b={b_large:.2f}\")\n",
    "\n",
    "                # Use the large-size fit if it has better scaling behavior\n",
    "                if b_large > b if simple_fit else True:\n",
    "                    fitted_params[algorithm] = (a_large, b_large, \"large_only\")\n",
    "                    print(\"  → Using large-size fit (better scaling)\")\n",
    "                elif simple_fit:\n",
    "                    fitted_params[algorithm] = (a, b, \"simple\")\n",
    "                    print(\"  → Using simple fit\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Large-size fitting failed: {e}\")\n",
    "                if simple_fit:\n",
    "                    fitted_params[algorithm] = (a, b, \"simple\")\n",
    "        elif simple_fit:\n",
    "            fitted_params[algorithm] = (a, b, \"simple\")\n",
    "\n",
    "print(f\"\\nSuccessfully fitted {len(fitted_params)} algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Performance Visualization with Union and Connected Operations\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== Creating Performance Visualizations ===\")\n",
    "\n",
    "# Convert results to Pandas DataFrames for easier plotting and analysis\n",
    "uf_df = pd.DataFrame(uf_results)\n",
    "threesum_df = pd.DataFrame(threesum_results)\n",
    "\n",
    "# Slope verification for complexity analysis\n",
    "print(\"\\n=== Slope Analysis for Complexity Verification ===\")\n",
    "\n",
    "# Styling constants for consistent and professional appearance\n",
    "MIN_DATA_POINTS = 2  # Minimum data points required for slope analysis\n",
    "LARGE_SIZE_TAIL = 3  # Number of largest points to use for large-size fits\n",
    "\n",
    "# Plot styling constants\n",
    "FIGURE_SIZE = (14, 16)  # Figure size for better readability\n",
    "LINE_WIDTH = 2  # Data line width\n",
    "FIT_LINE_WIDTH = 2  # Fitted curve line width\n",
    "FIT_ALPHA = 0.7  # Fitted curve transparency\n",
    "MARKER_SIZE = 6  # Consistent marker size\n",
    "GRID_ALPHA = 0.3  # Grid transparency\n",
    "LEGEND_FONTSIZE = 9  # Legend font size\n",
    "TITLE_FONTSIZE = 12  # Title font size\n",
    "LABEL_FONTSIZE = 10  # Axis label font size\n",
    "FIT_POINTS = 100  # Number of points for smooth fitted curves\n",
    "\n",
    "# Common plotting parameters to avoid repetition\n",
    "PLOT_PARAMS = {\n",
    "    \"markersize\": MARKER_SIZE,\n",
    "    \"linewidth\": LINE_WIDTH,\n",
    "    \"markeredgewidth\": 0.5,\n",
    "    \"markeredgecolor\": \"white\",\n",
    "}\n",
    "\n",
    "# UnionFind slope analysis for both operations\n",
    "print(\"\\nUnionFind Complexity Verification:\")\n",
    "for operation in [\"Union\", \"Connected\"]:\n",
    "    print(f\"\\n{operation} Operations:\")\n",
    "    operation_data = uf_df[uf_df[\"Operation\"] == operation]\n",
    "    for algorithm in operation_data[\"Algorithm\"].unique():\n",
    "        data = operation_data[operation_data[\"Algorithm\"] == algorithm].sort_values(\"N\")\n",
    "        if len(data) >= MIN_DATA_POINTS:\n",
    "            log_n = np.log10(data[\"N\"].values)\n",
    "            log_time = np.log10(data[\"Time (s)\"].values)\n",
    "            slope = (log_time[-1] - log_time[0]) / (log_n[-1] - log_n[0])\n",
    "            print(f\"  {algorithm}: slope = {slope:.2f}\")\n",
    "\n",
    "# 3Sum slope analysis\n",
    "print(\"\\n3Sum Find Operations Complexity Verification:\")\n",
    "for algorithm in threesum_df[\"Algorithm\"].unique():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm].sort_values(\"Array Size\")\n",
    "    if len(data) >= MIN_DATA_POINTS:\n",
    "        log_size = np.log10(data[\"Array Size\"].values)\n",
    "        log_time = np.log10(data[\"Time (s)\"].values)\n",
    "        slope = (log_time[-1] - log_time[0]) / (log_size[-1] - log_size[0])\n",
    "        print(f\"  {algorithm}: slope = {slope:.2f}\")\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=FIGURE_SIZE)\n",
    "\n",
    "# Define consistent color palette and styles for each algorithm\n",
    "colors = sns.color_palette(\"husl\", n_colors=4)\n",
    "uf_styles = {\n",
    "    \"Quick Find\": {\"color\": colors[0], \"marker\": \"o\", \"linestyle\": \"-\"},\n",
    "    \"Quick Union\": {\"color\": colors[1], \"marker\": \"s\", \"linestyle\": \"--\"},\n",
    "    \"Weighted Quick Union\": {\"color\": colors[2], \"marker\": \"^\", \"linestyle\": \"-.\"},\n",
    "    \"Weighted Quick Union with Path Compression\": {\n",
    "        \"color\": colors[3],\n",
    "        \"marker\": \"D\",\n",
    "        \"linestyle\": \":\",\n",
    "    },\n",
    "}\n",
    "\n",
    "threesum_colors = sns.color_palette(\"husl\", n_colors=3)\n",
    "threesum_styles = {\n",
    "    \"Brute Force\": {\"color\": threesum_colors[0], \"marker\": \"o\", \"linestyle\": \"-\"},\n",
    "    \"Optimized Two Pointers\": {\n",
    "        \"color\": threesum_colors[1],\n",
    "        \"marker\": \"s\",\n",
    "        \"linestyle\": \"--\",\n",
    "    },\n",
    "    \"Hash Set\": {\"color\": threesum_colors[2], \"marker\": \"^\", \"linestyle\": \"-.\"},\n",
    "}\n",
    "\n",
    "\n",
    "def plot_algorithm_data(data, x_col, y_col, styles, plot_func=plt.plot, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to plot algorithm data with consistent styling.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with algorithm performance data\n",
    "        x_col: Column name for x-axis data\n",
    "        y_col: Column name for y-axis data\n",
    "        styles: Dictionary mapping algorithm names to style dictionaries\n",
    "        plot_func: Plotting function to use (plt.plot, plt.loglog, etc.)\n",
    "        **kwargs: Additional parameters to pass to the plotting function\n",
    "    \"\"\"\n",
    "    for algorithm in data[\"Algorithm\"].unique():\n",
    "        algo_data = data[data[\"Algorithm\"] == algorithm]\n",
    "        style = styles[algorithm]\n",
    "        plot_func(\n",
    "            algo_data[x_col],\n",
    "            algo_data[y_col],\n",
    "            label=algorithm,\n",
    "            color=style[\"color\"],\n",
    "            marker=style[\"marker\"],\n",
    "            linestyle=style[\"linestyle\"],\n",
    "            **PLOT_PARAMS,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "def setup_subplot(title, xlabel, ylabel):\n",
    "    \"\"\"Helper function to set up subplot with consistent formatting.\"\"\"\n",
    "    plt.title(title, fontsize=TITLE_FONTSIZE, fontweight=\"bold\")\n",
    "    plt.xlabel(xlabel, fontsize=LABEL_FONTSIZE)\n",
    "    plt.ylabel(ylabel, fontsize=LABEL_FONTSIZE)\n",
    "    plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "    plt.grid(True, alpha=GRID_ALPHA)\n",
    "\n",
    "\n",
    "def setup_loglog_subplot(title, xlabel, ylabel):\n",
    "    \"\"\"Helper function to set up log-log subplot with consistent formatting.\"\"\"\n",
    "    setup_subplot(title, xlabel, ylabel)\n",
    "    plt.grid(True, alpha=GRID_ALPHA, which=\"both\")\n",
    "\n",
    "\n",
    "# Subplot 1: UnionFind Union Operations (Linear Scale)\n",
    "plt.subplot(3, 2, 1)\n",
    "union_data = uf_df[uf_df[\"Operation\"] == \"Union\"]\n",
    "plot_algorithm_data(union_data, \"N\", \"Time (s)\", uf_styles)\n",
    "setup_subplot(\n",
    "    \"UnionFind Union Operations\",\n",
    "    \"Number of Elements (N)\",\n",
    "    \"Time (seconds)\"\n",
    ")\n",
    "\n",
    "# Subplot 2: UnionFind Union Operations (Log-Log Scale)\n",
    "plt.subplot(3, 2, 2)\n",
    "plot_algorithm_data(union_data, \"N\", \"Time (s)\", uf_styles, plt.loglog)\n",
    "setup_loglog_subplot(\n",
    "    \"UnionFind Union Operations (Log-Log)\",\n",
    "    \"Number of Elements (N) - Log Scale\",\n",
    "    \"Time (seconds) - Log Scale\"\n",
    ")\n",
    "\n",
    "# Subplot 3: UnionFind Connected Operations (Linear Scale)\n",
    "plt.subplot(3, 2, 3)\n",
    "connected_data = uf_df[uf_df[\"Operation\"] == \"Connected\"]\n",
    "plot_algorithm_data(connected_data, \"N\", \"Time (s)\", uf_styles)\n",
    "setup_subplot(\n",
    "    \"UnionFind Connected Operations\",\n",
    "    \"Number of Elements (N)\",\n",
    "    \"Time (seconds)\"\n",
    ")\n",
    "\n",
    "# Subplot 4: UnionFind Connected Operations (Log-Log Scale)\n",
    "plt.subplot(3, 2, 4)\n",
    "plot_algorithm_data(connected_data, \"N\", \"Time (s)\", uf_styles, plt.loglog)\n",
    "setup_loglog_subplot(\n",
    "    \"UnionFind Connected Operations (Log-Log)\",\n",
    "    \"Number of Elements (N) - Log Scale\",\n",
    "    \"Time (seconds) - Log Scale\"\n",
    ")\n",
    "\n",
    "# Subplot 5: 3Sum Operations (Linear Scale)\n",
    "plt.subplot(3, 2, 5)\n",
    "plot_algorithm_data(threesum_df, \"Array Size\", \"Time (s)\", threesum_styles)\n",
    "\n",
    "# Add curve-fit overlays for 3Sum algorithms\n",
    "for algorithm, (a, b, fit_type) in fitted_params.items():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm].sort_values(\"Array Size\")\n",
    "    style = threesum_styles[algorithm]\n",
    "\n",
    "    if fit_type == \"large_only\":\n",
    "        # Use only the largest 3 points for the fit range\n",
    "        x_range = data.tail(LARGE_SIZE_TAIL)[\"Array Size\"]\n",
    "        x_fit = np.linspace(x_range.min(), x_range.max(), FIT_POINTS)\n",
    "    else:\n",
    "        # Use full range\n",
    "        x_fit = np.linspace(\n",
    "            data[\"Array Size\"].min(), data[\"Array Size\"].max(), FIT_POINTS\n",
    "        )\n",
    "\n",
    "    y_fit = power_law(x_fit, a, b)\n",
    "    plt.plot(\n",
    "        x_fit,\n",
    "        y_fit,\n",
    "        linestyle=\":\",\n",
    "        color=style[\"color\"],\n",
    "        alpha=FIT_ALPHA,\n",
    "        linewidth=FIT_LINE_WIDTH,\n",
    "        label=f\"{algorithm} fit (b={b:.1f})\",\n",
    "    )\n",
    "\n",
    "setup_subplot(\"3Sum Operations\", \"Array Size\", \"Time (seconds)\")\n",
    "\n",
    "# Subplot 6: 3Sum Operations (Log-Log Scale)\n",
    "plt.subplot(3, 2, 6)\n",
    "plot_algorithm_data(threesum_df, \"Array Size\", \"Time (s)\", threesum_styles, plt.loglog)\n",
    "\n",
    "# Add curve-fit overlays for 3Sum algorithms (Log-Log scale)\n",
    "for algorithm, (a, b, fit_type) in fitted_params.items():\n",
    "    data = threesum_df[threesum_df[\"Algorithm\"] == algorithm].sort_values(\"Array Size\")\n",
    "    style = threesum_styles[algorithm]\n",
    "\n",
    "    if fit_type == \"large_only\":\n",
    "        # Use only the largest 3 points for the fit range\n",
    "        x_range = data.tail(LARGE_SIZE_TAIL)[\"Array Size\"]\n",
    "        x_fit = np.linspace(x_range.min(), x_range.max(), FIT_POINTS)\n",
    "    else:\n",
    "        # Use full range\n",
    "        x_fit = np.linspace(\n",
    "            data[\"Array Size\"].min(), data[\"Array Size\"].max(), FIT_POINTS\n",
    "        )\n",
    "\n",
    "    y_fit = power_law(x_fit, a, b)\n",
    "    plt.loglog(\n",
    "        x_fit,\n",
    "        y_fit,\n",
    "        linestyle=\":\",\n",
    "        color=style[\"color\"],\n",
    "        alpha=FIT_ALPHA,\n",
    "        linewidth=FIT_LINE_WIDTH,\n",
    "        label=f\"{algorithm} fit (b={b:.1f})\",\n",
    "    )\n",
    "\n",
    "setup_loglog_subplot(\n",
    "    \"3Sum Operations (Log-Log)\",\n",
    "    \"Array Size - Log Scale\",\n",
    "    \"Time (seconds) - Log Scale\"\n",
    ")\n",
    "\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance visualizations with curve-fit overlays created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
